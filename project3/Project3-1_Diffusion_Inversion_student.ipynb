{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3-1: Inversion and sampling using diffusion models (55 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, given a real image (denoted as $x_0$) as input, we will invert this image into the diffusion model to get the initial noise (denoted as $x_T$). Then starting the diffusion sampling steps from this $x_T$, we can get a reconstruction $\\widetilde{x}_0$ of $x_0$. We will use DDIM (Denoising Diffusion Implicit Models)[1] as the diffusion sampling method. To perform inversion, we will perform DDIM sampling in reverse order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Before you start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, you need to understand: <br>\n",
    "1. The definition of forward and backward process in diffusion models. You can check the tutorial in [2]. You should also get familiar with the notations and symbols. However, you are not required to derive the formulas; <br>\n",
    "2. The definition of inversion using diffusion models; <br>\n",
    "3. Stable Diffusion [3] and its architecture [4], as we will use Stable Diffusion as our backbone model; <br>\n",
    "4. The diffusers library [5]. Please check the \"Get started\" section in the link [4] to install diffusers library. Please also go through \"Tutorial\" section to learn its basic usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Song, Jiaming, Chenlin Meng, and Stefano Ermon. \"Denoising diffusion implicit models.\" arXiv preprint arXiv:2010.02502 (2020). <br>\n",
    "[2] https://lilianweng.github.io/posts/2021-07-11-diffusion-models/ <br>\n",
    "[3] https://github.com/Stability-AI/stablediffusion <br>\n",
    "[4] Rombach, Robin, et al. \"High-resolution image synthesis with latent diffusion models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022. <br>\n",
    "[5] https://huggingface.co/docs/diffusers/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Get Started (2.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you install pytorch>=1.7, numpy, PIL, transformers and diffusers. <br>\n",
    "Possible library versions: <br>\n",
    "pytorch==1.12.1 <br>\n",
    "transformers==4.28.1 <br>\n",
    "diffusers==0.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diffusers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DDIMScheduler\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StableDiffusionPipeline\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Supress some unnecessary warnings when loading the CLIPTextModel\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'diffusers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from diffusers import DDIMScheduler\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# Supress some unnecessary warnings when loading the CLIPTextModel\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Set device\n",
    "torch_device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scheduler and model pipeline from diffusers library\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False,\n",
    "                                     set_alpha_to_one=False)\n",
    "sdm_model = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", scheduler=scheduler).to(torch_device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement DDIM method (30 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implement both DDIM inversion and sampling in a class called `DDIMMethod`. In the generation stage, we continuously apply formula (12) in DDIM paper [1], starting from $x_T$ and ending at $x_0$. For inversion, we apply the same formula but we start from $x_0$ and end at $x_T$. <br>\n",
    "Please follow the instructions below and fill out the missing lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, height=512, width=512):\n",
    "    image = Image.open(image_path).convert('RGB').resize((height, width))\n",
    "    image = np.array(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "class DDIMMethod(): \n",
    "    def __init__(self, model, num_ddim_steps=50):\n",
    "        self.model = model\n",
    "        self.num_ddim_steps = num_ddim_steps\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.model.scheduler.set_timesteps(self.num_ddim_steps)\n",
    "        self.total_steps = len(self.model.scheduler.timesteps)  # Total number of sampling steps.\n",
    "        print('Total number of steps: ', self.total_steps)\n",
    "        self.prompt = None\n",
    "        self.context = None\n",
    "    \n",
    "    # Please implement formula (12) in the DDIM paper. \n",
    "    # We set sigma_t = 0 so some terms can be ignored.\n",
    "    def next_step(self, noise_pred: torch.FloatTensor, timestep: int, latent: torch.FloatTensor, mode='inversion'):\n",
    "        # alpha_prod_t, alpha_prod_t_next and beta_prod_t are already calculated for you.\n",
    "        # Use these variables for the TODO tasks.\n",
    "        if mode == 'inversion':\n",
    "            timestep, next_timestep = min(timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps, 999), timestep\n",
    "            alpha_prod_t = self.scheduler.alphas_cumprod[timestep] if timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "            alpha_prod_t_next = self.scheduler.alphas_cumprod[next_timestep]\n",
    "\n",
    "        elif mode == 'sampling':\n",
    "            prev_timestep = timestep - self.scheduler.config.num_train_timesteps // self.scheduler.num_inference_steps\n",
    "            alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
    "            alpha_prod_t_next = self.scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else self.scheduler.final_alpha_cumprod\n",
    "                  \n",
    "        beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "        next_original_sample =  # TODO: \n",
    "        next_sample_direction =  # TODO:\n",
    "        next_sample =  # TODO:\n",
    "        return next_sample\n",
    "    \n",
    "\n",
    "    # Decode the latent to image using the VAE.\n",
    "    @torch.no_grad()\n",
    "    def latent2image(self, latents, return_type='np'):\n",
    "        latents = 1 / 0.18215 * latents.detach()\n",
    "        image = self.model.vae.decode(latents)['sample']\n",
    "        if return_type == 'np':\n",
    "            image = (image / 2 + 0.5).clamp(0, 1)\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "        return image\n",
    "\n",
    "    # Encode the image to latent using the VAE.\n",
    "    @torch.no_grad()\n",
    "    def image2latent(self, image):\n",
    "        with torch.no_grad():\n",
    "            if type(image) is Image:\n",
    "                image = np.array(image)\n",
    "            if type(image) is torch.Tensor and image.dim() == 4:\n",
    "                latents = image\n",
    "            else:\n",
    "                image = torch.from_numpy(image).float() / 127.5 - 1\n",
    "                image = image.permute(2, 0, 1).unsqueeze(0).to(self.model.device)\n",
    "                latents = self.model.vae.encode(image)['latent_dist'].mean\n",
    "                latents = latents * 0.18215\n",
    "        return latents\n",
    "\n",
    "    # Prepare the conditional embeddings\n",
    "    # Uncond_embeddings are the embeddings of the empty string,\n",
    "    # while the cond_embeddings are the embeddings of the input prompt.\n",
    "    @torch.no_grad()\n",
    "    def init_prompt(self, prompt: str):\n",
    "        uncond_input = self.model.tokenizer(\n",
    "            [\"\"], padding=\"max_length\", max_length=self.model.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        uncond_embeddings = self.model.text_encoder(uncond_input.input_ids.to(self.model.device))[0]\n",
    "        text_input = self.model.tokenizer(\n",
    "            [prompt],\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.model.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_embeddings = self.model.text_encoder(text_input.input_ids.to(self.model.device))[0]\n",
    "        self.context = torch.cat([uncond_embeddings, text_embeddings])\n",
    "        self.prompt = prompt\n",
    "\n",
    "    # Please implement the DDIM Loop by continuously applying formula (12). \n",
    "    # We will perform this steps for `self.total_steps` times.\n",
    "    # Note that the timesteps saved in `self.model.scheduler.timesteps` are from $T$ to $1$, not $1$ to $T$.\n",
    "    @torch.no_grad()\n",
    "    def ddim_loop(self, latent, mode='inversion'):\n",
    "        _, cond_embeddings = self.context.chunk(2) # We only use conditional embeddings here.\n",
    "        all_latents = [latent]  # We append $x_0$ to the start of the latent list.\n",
    "        latent = latent.clone().detach()\n",
    "        for i in range(self.total_steps):\n",
    "            if mode == 'inversion':\n",
    "                t = self.model.scheduler.timesteps[] # TODO: fill in the correct index.\n",
    "            elif mode == 'sampling':\n",
    "                t = self.model.scheduler.timesteps[] # TODO: fill in the correct index.\n",
    "            noise_pred =  # TODO: refer to the diffusers to see the usage of self.model.unet. \n",
    "                          # Here we only use cond_embeddings as the encoder_hidden_states.\n",
    "            latent =  # TODO: use self.next_step to get the next latent.\n",
    "            all_latents.append(latent)\n",
    "        return all_latents\n",
    "\n",
    "    @property\n",
    "    def scheduler(self):\n",
    "        return self.model.scheduler\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def invert(self, image_path: str, prompt: str):\n",
    "        self.init_prompt(prompt)\n",
    "        image_gt = load_image(image_path)\n",
    "        latent = self.image2latent(image_gt)\n",
    "        ddim_latents = self.ddim_loop(latent, mode='inversion')\n",
    "\n",
    "        return ddim_latents\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, latent: torch.Tensor, prompt: str):\n",
    "        self.init_prompt(prompt)\n",
    "        ddim_latents = self.ddim_loop(latent, mode='sampling')\n",
    "        \n",
    "        return ddim_latents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Invert an image (5 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim_method = DDIMMethod(sdm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real image inversion\n",
    "image_path = \"./inversion_imgs/cat_dog.jpg\"\n",
    "inversion_prompt = \"Photo of a cat and a dog\"  # prompt can be empty or a description of the image.\n",
    "mode = \"inversion\"\n",
    "ddim_latents = ddim_method.invert(image_path, inversion_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize \n",
    "# Please append x_0, x_10, x_20, x_30, x_40, x_50 to the image_inversion_list. Please use this order.\n",
    "image_inversion_list = []\n",
    "for i in range(0, 51, 10):\n",
    "    image_inversion = ddim_method.latent2image()  # TODO: fill in the correct latent from ddim_latents.\n",
    "    image_inversion = Image.fromarray(image_inversion)\n",
    "    image_inversion_list.append(image_inversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a list of pil images\n",
    "def visualize(image_list):\n",
    "    width, height = image_list[0].size\n",
    "    new_im = Image.new('RGB', (width * len(image_list), height))\n",
    "    for i, image in enumerate(image_list):\n",
    "        new_im.paste(image, (i * width, 0))\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The images from left to right should be x_0, x_10, x_20, x_30, x_40, x_50.\n",
    "visualize(image_inversion_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Test the inverted latent (7.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know that if the inverted $x_T$ we obtain is good or not. Therefore, we need to perform diffusion sampling steps to go from $x_T$ to the reconstructed $\\widetilde{x}_0$, and compare $\\widetilde{x}_0$ with $x_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We test if our reconstructed image is close to the original image.\n",
    "input_latent = ddim_latents[] # TODO: fill in the correct index.\n",
    "prompt = inversion_prompt # We have to use the same prompt as the one used in the inversion in order to reconstruct the image.\n",
    "ddim_latents = ddim_method.sample(input_latent, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sampling_list = []\n",
    "for i in range(0, 51, 10):\n",
    "    image_sampling = ddim_method.latent2image()  # TODO: fill in the correct latent from ddim_latents.\n",
    "    image_sampling = Image.fromarray(image_sampling)\n",
    "    image_sampling_list.append(image_sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The images from left to right should be x_50, x_40, x_30, x_20, x_10, x_0.\n",
    "# We can see that the reconstructed image is very close to the original image.\n",
    "visualize(image_sampling_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use PSNR to evaluate the inverted image quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(image1, image2):\n",
    "    mse = np.mean((image1 - image2) ** 2)\n",
    "    if mse - 0 < 1e-9:\n",
    "        print(\"Same images!\")\n",
    "    else:\n",
    "        psnr = 20 * np.log10(255.0 / np.sqrt(mse))\n",
    "        print(\"PSNR: {}\".format(psnr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: please run the cell and report the PSNR.\n",
    "original_image = image_inversion_list[0]\n",
    "inverted_image = image_sampling_list[-1]\n",
    "\n",
    "calculate_psnr(np.array(inverted_image), np.array(original_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Try your own image! (10 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Invert your one own image. Follow the same procedure as above and visualize $x_{0}, x_{10}, x_{20}, x_{30}, x_{40}, x_{50}$ from left to right. Please use a prompt that describes your input image. <br>\n",
    "Step 2: Reconstruct the image from the inverted latent. Follow the same procedure as above and visualize $x_{50}, x_{40}, x_{30}, x_{20}, x_{10}, x_{0}$ from left to right. Please use the same prompt as the inversion prompt.  <br>\n",
    "Step 3: Compute the PSNR between your original image and the inverted image. <br>\n",
    "<br>\n",
    "Please keep all the output in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
