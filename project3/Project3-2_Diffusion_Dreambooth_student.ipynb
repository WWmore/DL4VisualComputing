{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3-2: Dreambooth (45 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are going to implement an interesting work called Dreambooth [1]. Given a few images of a subject, Dreambooth finetunes the diffusion model and learns a new concept for that subject. After that, the new concept can be used in the finetuned diffusion model to generate new images of the given subject. We will use a few images of a corgi as an example, and \"personalize\" our own model to make it generate this corgi. In a word, we will get: <br>\n",
    "1. a new embedding for our concept; <br>\n",
    "2. a fintuned diffusion model. <br>\n",
    "\n",
    "Please refer to [1] for more details. Note that this implementation will have some differences from the original paper. Similar to Project 3-1, we will use Stable Diffusion as our backbone model and diffusers library for implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Ruiz, Nataniel, et al. \"Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.\" CVPR 2023. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Get started (2.5 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please make sure you install pytorch>=1.7, numpy, PIL, transformers and diffusers. <br>\n",
    "Possible library versions: <br>\n",
    "pytorch==1.12.1 <br>\n",
    "torchvision==0.13.1 <br>\n",
    "transformers==4.28.1 <br>\n",
    "diffusers==0.16.1 <br>\n",
    "You can basically install new packages based on the environment you use for Project 3-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import PIL\n",
    "from diffusers import AutoencoderKL, DDIMScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# suppress logs from transformers library\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run this cell and visualize the images we collected for the corgi.\n",
    "# These images will be used to finetune the diffusion model.\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "images = []\n",
    "image_dir = './dreambooth_imgs/corgi'\n",
    "for img_name in os.listdir(image_dir):    \n",
    "    image = Image.open(os.path.join(image_dir, img_name))\n",
    "    images.append(image.resize((512, 512)))\n",
    "\n",
    "image_grid(images, 1, len(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Prepare the data (2.5 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder_token = \"<cute-corgi>\"  # This is the new unique identifier for the corgi class.\n",
    "initializer_token = \"dog\"  # We initialize the new embedding with the embedding for the \"dog\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a collection of the text template for prompting.\n",
    "imagenet_templates_small = [\n",
    "    \"a photo of a {}\",\n",
    "    \"a rendering of a {}\",\n",
    "    \"a cropped photo of the {}\",\n",
    "    \"the photo of a {}\",\n",
    "    \"a photo of a clean {}\",\n",
    "    \"a photo of a dirty {}\",\n",
    "    \"a dark photo of the {}\",\n",
    "    \"a photo of my {}\",\n",
    "    \"a photo of the cool {}\",\n",
    "    \"a close-up photo of a {}\",\n",
    "    \"a bright photo of the {}\",\n",
    "    \"a cropped photo of a {}\",\n",
    "    \"a photo of the {}\",\n",
    "    \"a good photo of the {}\",\n",
    "    \"a photo of one {}\",\n",
    "    \"a close-up photo of the {}\",\n",
    "    \"a rendition of the {}\",\n",
    "    \"a photo of the clean {}\",\n",
    "    \"a rendition of a {}\",\n",
    "    \"a photo of a nice {}\",\n",
    "    \"a good photo of a {}\",\n",
    "    \"a photo of the nice {}\",\n",
    "    \"a photo of the small {}\",\n",
    "    \"a photo of the weird {}\",\n",
    "    \"a photo of the large {}\",\n",
    "    \"a photo of a cool {}\",\n",
    "    \"a photo of a small {}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set up the dataset for training\n",
    "class DreamboothDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root,\n",
    "        tokenizer,\n",
    "        size=512,\n",
    "        repeats=100,\n",
    "        interpolation=\"bicubic\",\n",
    "        flip_p=0.5,\n",
    "        set=\"train\",\n",
    "        placeholder_token=\"*\",\n",
    "        center_crop=False,\n",
    "    ):\n",
    "\n",
    "        self.data_root = data_root\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.placeholder_token = placeholder_token\n",
    "        self.center_crop = center_crop\n",
    "        self.flip_p = flip_p\n",
    "\n",
    "        self.image_paths = [os.path.join(self.data_root, file_path) for file_path in os.listdir(self.data_root)]\n",
    "\n",
    "        self.num_images = len(self.image_paths)\n",
    "        self._length = self.num_images\n",
    "\n",
    "        if set == \"train\":\n",
    "            self._length = self.num_images * repeats\n",
    "\n",
    "        self.interpolation = {\n",
    "            \"linear\": PIL.Image.LINEAR,\n",
    "            \"bilinear\": PIL.Image.BILINEAR,\n",
    "            \"bicubic\": PIL.Image.BICUBIC,\n",
    "            \"lanczos\": PIL.Image.LANCZOS,\n",
    "        }[interpolation]\n",
    "\n",
    "        self.templates = imagenet_templates_small\n",
    "        self.flip_transform = transforms.RandomHorizontalFlip(p=self.flip_p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = {}\n",
    "        image = Image.open(self.image_paths[i % self.num_images])\n",
    "\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        placeholder_string = self.placeholder_token\n",
    "        # TODO: we use package `random` to randomly choose a text template from self.templates, \n",
    "        # and fill the place holder with placeholder_string.\n",
    "        text = \n",
    "\n",
    "        example[\"input_ids\"] = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids[0]\n",
    "\n",
    "        # default to score-sde preprocessing\n",
    "        img = np.array(image).astype(np.uint8)\n",
    "\n",
    "        if self.center_crop:\n",
    "            crop = min(img.shape[0], img.shape[1])\n",
    "            h, w, = (\n",
    "                img.shape[0],\n",
    "                img.shape[1],\n",
    "            )\n",
    "            img = img[(h - crop) // 2 : (h + crop) // 2, (w - crop) // 2 : (w + crop) // 2]\n",
    "\n",
    "        image = Image.fromarray(img)\n",
    "        image = image.resize((self.size, self.size), resample=self.interpolation)\n",
    "\n",
    "        image = self.flip_transform(image)\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "\n",
    "        example[\"pixel_values\"] = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Set up training (20 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\"\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    subfolder=\"tokenizer\",\n",
    ")\n",
    "\n",
    "# Add the placeholder token in tokenizer\n",
    "num_added_tokens = tokenizer.add_tokens(placeholder_token)\n",
    "if num_added_tokens == 0:\n",
    "    raise ValueError(\n",
    "        f\"The tokenizer already contains the token {placeholder_token}. Please pass a different\"\n",
    "        \" `placeholder_token` that is not already in the tokenizer.\"\n",
    "    )\n",
    "# Get the token id of the placeholder token\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
    "\n",
    "# We don't need to add intializer_token to the tokenizer since it is already in the vocabulary\n",
    "token_ids = tokenizer.encode(initializer_token, add_special_tokens=False)\n",
    "# Get the token id of the initializer token\n",
    "initializer_token_id = token_ids[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"unet\"\n",
    ")\n",
    "noise_scheduler = DDIMScheduler.from_config(pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "token_embeds = text_encoder.get_input_embeddings().weight.data\n",
    "# Initialize the placeholder token embedding with the initializer token embedding\n",
    "token_embeds[placeholder_token_id] = token_embeds[initializer_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_params(params):\n",
    "    for param in params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Freeze vae\n",
    "freeze_params(vae.parameters())\n",
    "# Freeze all parameters in the text encoder except for the token embeddings\n",
    "params_to_freeze = itertools.chain(\n",
    "    text_encoder.text_model.encoder.parameters(),\n",
    "    text_encoder.text_model.final_layer_norm.parameters(),\n",
    "    text_encoder.text_model.embeddings.position_embedding.parameters(),\n",
    ")\n",
    "freeze_params(params_to_freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DreamboothDataset(\n",
    "      data_root=image_dir,\n",
    "      tokenizer=tokenizer,\n",
    "      size=vae.sample_size,\n",
    "      placeholder_token=placeholder_token,\n",
    "      repeats=100,\n",
    "      center_crop=False,\n",
    "      set=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the missing lines in the training function\n",
    "def train(text_encoder, vae, unet, hyperparameters):\n",
    "    train_batch_size = hyperparameters[\"train_batch_size\"]\n",
    "    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"]\n",
    "    learning_rate = hyperparameters[\"learning_rate\"]\n",
    "    max_train_steps = hyperparameters[\"max_train_steps\"]\n",
    "    output_dir = hyperparameters[\"output_dir\"]\n",
    "    gradient_checkpointing = hyperparameters[\"gradient_checkpointing\"]\n",
    "    save_steps = hyperparameters[\"save_steps\"]\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        text_encoder.gradient_checkpointing_enable()\n",
    "        unet.enable_gradient_checkpointing()\n",
    "\n",
    "    # TODO: Create the dataloader using `train_dataset`\n",
    "    train_dataloader = \n",
    "\n",
    "    if hyperparameters[\"scale_lr\"]:\n",
    "        learning_rate = (\n",
    "            learning_rate * gradient_accumulation_steps * train_batch_size\n",
    "        )\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(text_encoder.get_input_embeddings().parameters()) + list(unet.parameters()),  # only optimize the embeddings\n",
    "        lr=learning_rate,\n",
    "    )\n",
    "\n",
    "    # Move vae and unet to device\n",
    "    vae.to(device)\n",
    "    unet.to(device)\n",
    "    text_encoder.to(device)\n",
    "\n",
    "    # Keep vae in eval mode as we don't train it\n",
    "    vae.eval()\n",
    "    # Keep unet in train mode to enable gradient checkpointing\n",
    "    unet.train()\n",
    "\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    total_batch_size = train_batch_size * gradient_accumulation_steps\n",
    "\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(max_train_steps))\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        text_encoder.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Convert images to latent space\n",
    "            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "            batch[\"input_ids\"] = batch[\"input_ids\"].to(device)\n",
    "            \n",
    "            latents = vae.encode(batch[\"pixel_values\"]).latent_dist.sample().detach()\n",
    "            latents = latents * 0.18215\n",
    "\n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise =  # TODO: sample a random noise. What should be the shape of the noise?\n",
    "            bsz = latents.shape[0]\n",
    "\n",
    "            timesteps = # TODO: Sample a random integer as the timestep for each image. \n",
    "                        # Hint: use `torch.randint()`.\n",
    "                        # The shape of the tensor should be (bsz, ).\n",
    "            \n",
    "            # Add noise to the latents\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "            noise_pred = # TODO: Predict the noise residual\n",
    "\n",
    "            # Get the target for loss depending on the prediction type\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                target = noise\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "            \n",
    "            loss = # TODO: Calculate the MSE loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Zero out the gradients for all token embeddings except the newly added\n",
    "            # embeddings for the concept, as we only want to optimize the concept embeddings\n",
    "            grads = text_encoder.get_input_embeddings().weight.grad\n",
    "            # Get the index for tokens that we want to zero the grads for\n",
    "            index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "            # Zero out the grads at the index_grads_to_zero. \n",
    "            grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item()}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "            \n",
    "            if global_step % save_steps == 0:\n",
    "                # Create the pipeline using using the trained modules and save it.\n",
    "                pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "                    pretrained_model_name_or_path,\n",
    "                    text_encoder=text_encoder,\n",
    "                    tokenizer=tokenizer,\n",
    "                    vae=vae,\n",
    "                    unet=unet,\n",
    "                )\n",
    "                output_path = os.path.join(output_dir, \"checkpoint-{}_{}\".format(global_step, hyperparameters[\"learning_rate\"]))\n",
    "                pipeline.save_pretrained(output_path)\n",
    "                # Also save the newly trained embeddings\n",
    "\n",
    "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        vae=vae,\n",
    "        unet=unet,\n",
    "    )\n",
    "    output_path = os.path.join(output_dir, \"checkpoint-{}_{}\".format(global_step, hyperparameters[\"learning_rate\"]))\n",
    "    pipeline.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up all training hyperparameters.\n",
    "# You can keep the other hyperparameters as they are, \n",
    "# but you can also try to change them to see if you can get better results.\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": ,  # TODO: try to find a good setting between 1e-7 and 1e-6\n",
    "    \"scale_lr\": True,\n",
    "    \"max_train_steps\": ,  # TODO: try to find a good setting between 50 and 500\n",
    "    \"save_steps\": 100,  # We save the intermediate pipeline every save_steps steps\n",
    "    \"train_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"mixed_precision\": \"fp16\",\n",
    "    \"seed\": 20,\n",
    "    \"output_dir\": \"./output/dreambooth_corgi\"\n",
    "}\n",
    "train(text_encoder, vae, unet, hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in itertools.chain(unet.parameters(), text_encoder.parameters()):\n",
    "  if param.grad is not None:\n",
    "    del param.grad  # free some memory\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Test our new concept! (20 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What good output images should look like if you use your new concept of corgi for testing? <br>\n",
    "1. The images should be of the same visual quality as the other Stable Diffusion images. <br>\n",
    "2. The images should contain the corgi. <br>\n",
    "3. The images should not look too similar as the training images. <br>\n",
    "4. The images should correctly show the content of your prompt. (At least when your prompt is simple) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting can easily happen especially when the training is too long. Here are some overfitting examples using the prompt \"Photo of a \\<cute-corgi\\> swimming\": <br>\n",
    "<img src=\"exp_output/corgi_swimming.jpg\" alt=\"Overfitting\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline we have trained.\n",
    "steps =  # TODO: please decide which checkpoint to load\n",
    "learning_rate = hyperparameters[\"learning_rate\"]\n",
    "pipe_path = os.path.join(hyperparameters[\"output_dir\"], \"checkpoint-{}_{}\".format(steps, learning_rate))\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    pipe_path,\n",
    "    scheduler=DDIMScheduler.from_pretrained(pipe_path, subfolder=\"scheduler\"),\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cell and see if the generated images include the corgi in the reference images.\n",
    "# If you want to train the model with new hyperparameters, \n",
    "# consider restart the runtime and run all cells again.\n",
    "prompt = \"Photo of a {} running on the beach\".format(placeholder_token)\n",
    "\n",
    "num_samples = 4\n",
    "num_rows = 2\n",
    "\n",
    "new_images = [] \n",
    "for _ in range(num_rows):\n",
    "    images = pipe([prompt] * num_samples, num_inference_steps=50, guidance_scale=7.5).images\n",
    "    new_images.extend(images)\n",
    "\n",
    "grid = image_grid(new_images, num_rows, num_samples)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run this additional test to see \n",
    "# whether the model overfitts to the corgi so that it forgets \n",
    "# how to generate other subjects or not.\n",
    "prompt = \"Photo of a bear dancing in the forest\"\n",
    "\n",
    "num_samples = 4\n",
    "num_rows = 1\n",
    "\n",
    "new_images = [] \n",
    "for _ in range(num_rows):\n",
    "    images = pipe([prompt] * num_samples, num_inference_steps=50, guidance_scale=7.5).images\n",
    "    new_images.extend(images)\n",
    "\n",
    "grid = image_grid(new_images, num_rows, num_samples)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this additional test to see \n",
    "# whether the model overfitts to the corgi so that it forgets \n",
    "# how to generate subject of the same class as the corgi.\n",
    "# In this case, the same class should be \"dog\".\n",
    "# It's ok that you see some corgi-like dogs in the generated images.\n",
    "prompt = \"Photo of a dog running on the beach\"\n",
    "\n",
    "num_samples = 4\n",
    "num_rows = 1\n",
    "\n",
    "new_images = [] \n",
    "for _ in range(num_rows):\n",
    "    images = pipe([prompt] * num_samples, num_inference_steps=50, guidance_scale=7.5).images\n",
    "    new_images.extend(images)\n",
    "\n",
    "grid = image_grid(new_images, num_rows, num_samples)\n",
    "grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
