{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quHwuvnL_N9o"
   },
   "source": [
    "# Project 1: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JWfOd3AVO9J"
   },
   "source": [
    "## Task 0: Getting Started\n",
    "\n",
    "Read the getting started guide titled **\"Python for Deep Learning\"** and get familiar with Python and PyTorch. Read the provided code below and get familiar with the commands and their parameters to understand what the code is trying to do. We recommend to spend a fair amount of time to understand all the different parts of the code. This understanding will be important for this and future projects.\n",
    "\n",
    "The goal of this project is to implement the *“Hello World!”* program of deep learning: designing and training a network that performs image classification. The dataset we will be using is CIFAR10 which is a large set of images that are classified into 10 classes (airplane, bird, cat, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wgsyd2YsVO9L"
   },
   "source": [
    "## Task 1:  Data Loading (10 points)\n",
    "Complete the **DataLoader** below which we will use to load images of the cifar10 dataset provided by torchvision. Your task is to normalize it by shifting and scaling it by a factor of 0.5. For the training set, introduce random transformations (e.g. flips) for data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EtxYeHjRVO9S",
    "outputId": "210857dc-3b3e-4bd2-cb61-ae1e13877715"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "print(torch.__version__)\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "# Data augmentation, tensor conversion and normalization for training\n",
    "# Just normalization and tensor conversion for testing\n",
    "mean, std = (0.5,), (0.5,)\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([transforms.RandomHorizontalFlip(), \n",
    "                                 transforms.ToTensor(), \n",
    "                                 transforms.Normalize(mean, std)]),\n",
    "    'test': transforms.Compose([transforms.RandomHorizontalFlip(), \n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize(mean, std)])\n",
    "}\n",
    "\n",
    "# Download (if not downloaded before) & Load CIFAR10\n",
    "torch.random.manual_seed(0)\n",
    "image_datasets = {x: torchvision.datasets.CIFAR10(root='./data', train=(x=='train'), download=True, transform=data_transforms[x]) for x in ['train', 'test']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=(x=='train'), num_workers=4) for x in ['train', 'test']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Ship to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(f\"The classes found in CIFAR-10 are: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uS6cR4RhVO9U"
   },
   "source": [
    "### Visualize a few images\n",
    "\n",
    "Let’s visualize a few training images so as to understand the data augmentations. The results should look like:\n",
    "\n",
    "<img src=\"https://i.imgur.com/Sa6l1go.png\" width=\"400\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xXSA5DDBVO9V"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO Task 1:  Run this cell and try to understand the output of each step\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmMMSK0TVO9L"
   },
   "source": [
    "## Task 2: Basic Networks (20 points)\n",
    "1. Create a Fully connected Network (FcNet) using the following layers in the Jupyter Notebook (use the non-linearities wherever necessary):\n",
    "```\n",
    "FcNet(\n",
    "  (fc1): Linear(in_features=3072, out_features=1024, bias=True)\n",
    "  (fc2): Linear(in_features=1024, out_features=400, bias=True)\n",
    "  (fc3): Linear(in_features=400, out_features=84, bias=True)\n",
    "  (fc4): Linear(in_features=84, out_features=10, bias=True)\n",
    ")\n",
    "```\n",
    "Train the FcNet for **3** epoches and record the training time and accuracy in your final report.\n",
    "\n",
    "2. Create a Convolutional Network (ConvNet) using the following layers in the Jupyter Notebook (use the non-linearities wherever necessary):\n",
    "```\n",
    "ConvNet(\n",
    "  (conv): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (conv): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
    "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
    "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
    ")\n",
    "```\n",
    "Train the ConvNet for **3** epoches and record the training time and accuracy in your final report. Note: You can reuse the conv layers to match the in_features of fc1. \n",
    "\n",
    "*Use the default SGD optimizer ( lr=0.001, momentum=0.9) for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sKXg8fSVO9X"
   },
   "source": [
    "### Model training code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A_Y2cgPEVO9X"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=25, save_path='saved_weight.pth'):\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            if phase == 'train': model.train()  # Set model to training mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrQFn-EzBKmA"
   },
   "source": [
    "### Model test code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ttChDlePBKmB"
   },
   "outputs": [],
   "source": [
    "def test_model(model, load_path='saved_weight.pth'):    \n",
    "    # load the model weights\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    for phase in ['test']:\n",
    "        if phase == 'test':\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "           \n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # statistics\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        print('{} Acc: {:.4f}'.format(phase, epoch_acc))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Testing complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX6I8cqXFDMX"
   },
   "source": [
    "### 1) FC Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpompN92VO9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FcNet(\n",
      "  (fc1): Linear(in_features=3072, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=400, bias=True)\n",
      "  (fc3): Linear(in_features=400, out_features=84, bias=True)\n",
      "  (fc4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Epoch 0/2\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# 1) Define a Fully Connected Neural Network\n",
    "# Please advise the Pytorch Documentation and use the appropiate calls\n",
    "\n",
    "class FcNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FcNet, self).__init__()\n",
    "        # TODO Task 2:  Define the layers \n",
    "        self.fc1 = nn.Linear(in_features=3072, out_features=1024, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=400, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=400, out_features=84, bias=True)\n",
    "        self.fc4 = nn.Linear(in_features=84, out_features=10, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO Task 2:  Define the forward pass\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model_ft = FcNet() #Define the model\n",
    "model_ft = model_ft.to(device) \n",
    "print(model_ft)\n",
    "\n",
    "# TODO Task 2:  Define loss criterion - cross entropy loss\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# TODO Task 2:  Define Optimizer\n",
    "optimizer = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# TODO Task 2:  Train the model\n",
    "model_train = train_model(model_ft, criterion, optimizer, num_epochs=3, save_path='saved_FcNet_weight.pth')\n",
    "\n",
    "# TODO Task 2:  Test the model\n",
    "model_test = test_model(model_ft, load_path='saved_FcNet_weight.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFQdcJaKFF8y"
   },
   "source": [
    "### 2) CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ma36XwVfVO9h"
   },
   "outputs": [],
   "source": [
    "# 2) Define a Convolutional Neural Network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # TODO Task 2:  Define the CNN layers \n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
    "        self.fc1 = nn.Linear(in_features=400, out_features=120, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO Task 2:  Define the forward pass\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 400)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model_ft = ConvNet() #Define the model\n",
    "model_ft = model_ft.to(device)\n",
    "print(model_ft)\n",
    "\n",
    "# TODO Task 2:  Define loss criterion - cross entropy loss\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# TODO Task 2:  Define Optimizer\n",
    "optimizer = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# TODO Task 2:  Train the model\n",
    "model_train = train_model(model_ft, criterion, optimizer, num_epochs=3, save_path='saved_CNNNet_weight.pth')\n",
    "\n",
    "# TODO Task 2:  Test the model\n",
    "model_test = test_model(model_ft, load_path='saved_CNNNet_weight.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMAIB0f8VO9M"
   },
   "source": [
    "## Task 3.A: Design Your Network (30 points)\n",
    "Define your own Convolutional Network (MyNet) based on the [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf) architecture (see Sec. 4.2 for the original architecture used). Here we will experiment with different configurations. Add following modifications and train the Network for **25** epoches. Keep the best settings for each step (for each step, record the training accuracy of the last epoch and test accuracy in your report):\n",
    "\n",
    "- Modify the number of ResNet blocks per layer: For Simplicity, implement a three-layered ResNet architecture. For each layer, try 1 , 2 and 3 number of ResNet blocks (3 configurations in total). You can choose any of the downsampling methods to match the tensor sizes wherever necessary. Use 2D average pooling layer before applying the final linear layer. For the three layers keep the number of filters 16, 32 and 64 respectively. Follow the ResNet paper to select the kernel size, paddings, optimizer/learning rate, strides, activations and **Batch Normalization** (select a suitable batch size) layers for this task. Print the model summary of the selected model.\n",
    "\n",
    "#### *Bonus Points: Define a validation set within the training set to monitor underfitting/overfitting after every epoch for each task. (Hint modify dataloaders and/or train_model function) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Your Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.A: Configuration 1\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_block=3):\n",
    "        super(MyNet, self).__init__()\n",
    "        # TODO Task 3: Design Your Network\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.layer1 = ResNetBlock(16, 16)\n",
    "        self.layer2 = ResNetBlock(16, 32, stride=2)\n",
    "        self.layer3 = ResNetBlock(32, 64, stride=2)\n",
    "        if num_block>1:\n",
    "            self.layer1 = nn.Sequential(self.layer1,ResNetBlock(16, 16))\n",
    "            self.layer2 = nn.Sequential(self.layer2,ResNetBlock(32, 32))\n",
    "            self.layer3 = nn.Sequential(self.layer3,ResNetBlock(64, 64))\n",
    "        if num_block>2:\n",
    "            self.layer1 = nn.Sequential(self.layer1,ResNetBlock(16, 16))\n",
    "            self.layer2 = nn.Sequential(self.layer2,ResNetBlock(32, 32))\n",
    "            self.layer3 = nn.Sequential(self.layer3,ResNetBlock(64, 64))\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO Task 3: Design Your Network\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.A: Configuration 2\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                                          nn.BatchNorm2d(out_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 1.9865 Acc: 0.2707\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.8384 Acc: 0.3171\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.7586 Acc: 0.3548\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.7311 Acc: 0.3666\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 1.7097 Acc: 0.3779\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 1.7060 Acc: 0.3828\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 1.6997 Acc: 0.3845\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 1.7071 Acc: 0.3778\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 1.7029 Acc: 0.3832\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 1.7021 Acc: 0.3845\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 1.7079 Acc: 0.3803\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 1.7028 Acc: 0.3854\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 1.7078 Acc: 0.3796\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 1.7199 Acc: 0.3750\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 1.7184 Acc: 0.3757\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 1.7274 Acc: 0.3738\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 1.7347 Acc: 0.3714\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 1.7372 Acc: 0.3700\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 1.7273 Acc: 0.3731\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 1.7389 Acc: 0.3697\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 1.7754 Acc: 0.3550\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 1.7697 Acc: 0.3536\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 1.7564 Acc: 0.3636\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 1.7664 Acc: 0.3607\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 1.7648 Acc: 0.3633\n",
      "Training complete in 188m 4s\n",
      "test Acc: 0.2919\n",
      "Testing complete in 0m 39s\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 2.0710 Acc: 0.2299\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.8717 Acc: 0.3084\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.7788 Acc: 0.3463\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.7480 Acc: 0.3632\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 1.7279 Acc: 0.3736\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 1.7259 Acc: 0.3732\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 1.7237 Acc: 0.3762\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 1.7177 Acc: 0.3763\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 1.7223 Acc: 0.3752\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 1.7210 Acc: 0.3778\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 1.7234 Acc: 0.3764\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 1.7235 Acc: 0.3755\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 1.7230 Acc: 0.3726\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 1.7296 Acc: 0.3736\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 1.7268 Acc: 0.3720\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 1.7382 Acc: 0.3709\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 1.7573 Acc: 0.3641\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 1.7576 Acc: 0.3647\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 1.7662 Acc: 0.3623\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 1.7911 Acc: 0.3529\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 1.7864 Acc: 0.3558\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 1.7974 Acc: 0.3507\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 1.7878 Acc: 0.3501\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 1.7815 Acc: 0.3585\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 1.7899 Acc: 0.3546\n",
      "Training complete in 315m 41s\n",
      "test Acc: 0.2766\n",
      "Testing complete in 1m 6s\n",
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 2.0790 Acc: 0.2260\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 1.9336 Acc: 0.2841\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 1.8829 Acc: 0.3026\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 1.8560 Acc: 0.3120\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 1.8277 Acc: 0.3261\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 1.7954 Acc: 0.3402\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 1.7758 Acc: 0.3513\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 1.7609 Acc: 0.3579\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 1.7413 Acc: 0.3645\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 1.7259 Acc: 0.3711\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 1.7112 Acc: 0.3806\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 1.7165 Acc: 0.3779\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 1.7177 Acc: 0.3758\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 1.7103 Acc: 0.3808\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 1.7263 Acc: 0.3726\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 1.7257 Acc: 0.3762\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 1.7249 Acc: 0.3782\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 1.7145 Acc: 0.3789\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 1.7197 Acc: 0.3806\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 1.7178 Acc: 0.3793\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 1.7171 Acc: 0.3801\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 1.7257 Acc: 0.3774\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 1.7284 Acc: 0.3743\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 1.7250 Acc: 0.3777\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 1.7329 Acc: 0.3753\n",
      "Training complete in 484m 3s\n",
      "test Acc: 0.3574\n",
      "Testing complete in 1m 20s\n"
     ]
    }
   ],
   "source": [
    "# Task 3.A: Configuration 3\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=data_transforms)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=data_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in range(3):\n",
    "    model = MyNet(num_block=i+1).to(device) ## choose num_block=1,2,3\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    model_train = train_model(model, criterion, optimizer, num_epochs=25, save_path='saved_ResNet_Block'+str(i+1)+'_weight.pth')\n",
    "    model_test = test_model(model, load_path='saved_ResNet_Block'+str(i+1)+'_weight.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.B: Design Your Network (10 points)\n",
    "Using your best network/model from Task 3.A, please do the following modifications:\n",
    "\n",
    "- Use Dropout: Toggle **Dropout** in fully connected layers. Does it improve the validation/test accuracy and/or avoid overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 2.3686 Acc: 0.1001\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 2.3586 Acc: 0.1002\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 2.3601 Acc: 0.0978\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 2.3603 Acc: 0.0994\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 2.3583 Acc: 0.1003\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 2.3592 Acc: 0.0991\n",
      "Epoch 6/24\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Task 3.B: Using Dropout\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, num_block=3, dropout_rate=0.5): ##dropout_rate is newly added\n",
    "        super(MyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.layer1 = ResNetBlock(16, 16)\n",
    "        self.layer2 = ResNetBlock(16, 32, stride=2)\n",
    "        self.layer3 = ResNetBlock(32, 64, stride=2)\n",
    "        \n",
    "        if num_block==2:\n",
    "            self.layer1 = nn.Sequential(self.layer1,ResNetBlock(16, 16))\n",
    "            self.layer2 = nn.Sequential(self.layer2,ResNetBlock(32, 32))\n",
    "            self.layer3 = nn.Sequential(self.layer3,ResNetBlock(64, 64))\n",
    "            \n",
    "        elif num_block==3:\n",
    "            self.layer1 = nn.Sequential(self.layer1,ResNetBlock(16, 16),ResNetBlock(16, 16))\n",
    "            self.layer2 = nn.Sequential(self.layer2,ResNetBlock(32, 32),ResNetBlock(32, 32))\n",
    "            self.layer3 = nn.Sequential(self.layer3,ResNetBlock(64, 64),ResNetBlock(64, 64))\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate) ##newly added\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out) ##newly added\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = MyNet(num_block=3, dropout_rate=0.5).to(device) ##newly changed\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "model_train = train_model(model, criterion, optimizer, num_epochs=25, save_path='saved_Droupout_weight.pth')\n",
    "model_test = test_model(model, load_path='saved_Droupout_weight.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8E_4eLwBKmD"
   },
   "source": [
    "## Task 4: The Optimizer (20 points)\n",
    "Keeping the best settings of Task 4, try 2 different optimizers (SGD and ADAM) with 3 different learning rates (0.01, 0.1, 1.) . Keep the other parameters as default. Plot the loss curves ( Training step vs Training loss ) for the 6 cases (Hint: Modify the train_model fuction). How does the learning rate affect the training?\n",
    "\n",
    "#### *Bonus Points: Define a validation set within the training set to examine the best model among the above cases. (Hint modify dataloaders and/or train_model function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PpRAJHrtVO9o"
   },
   "outputs": [],
   "source": [
    "# Define a Convolutional Neural Network\n",
    "class MyFinalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        # TODO Task 3: Design Your Network\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO Task 3: Design Your Network\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "77S-thkZVO9q"
   },
   "outputs": [],
   "source": [
    "model = MyFinalNet() ##TODO\n",
    "model = model.to(device)\n",
    "\n",
    "# TODO:  Define loss criterion - cross entropy loss\n",
    "criterion = F.cross_entropy\n",
    "\n",
    "# TODO Task 4: The Optimizer\n",
    "def configure_optimizers(model, name='SGD', rate=0.01):\n",
    "    if name=='SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=rate, momentum=0.9, weight_decay=5e-4)\n",
    "    elif name=='ADAM'\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=rate, weight_decay=5e-4)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "## Train and evaluate\n",
    "def train_model(model, optimizer, criterion, train_loader, test_loader, num_epochs=25):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_loss /= len(train_loader.dataset)\n",
    "            train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "                test_loss /= len(test_loader.dataset)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "rates = [0.01, 0.1, 1.0]\n",
    "\n",
    "for lr in rates:\n",
    "    \"SGD optimizer with learning rates of 0.01, 0.1, and 1.0\"\n",
    "    optimizer = configure_optimizers(model, name='SGD', rate=lr)\n",
    "    train_losses, test_losses = train_model(model, optimizer, criterion, train_loader, test_loader)\n",
    "    plt.plot(train_losses, label=f\"SGD lr={lr}\")\n",
    "\n",
    "for lr in rates:\n",
    "    \"ADAM optimizer with learning rates of 0.01, 0.1, and 1.0\"\n",
    "    optimizer = configure_optimizers(model, name='ADAM', rate=lr)\n",
    "    train_losses, test_losses = train_model(model, optimizer, criterion, train_loader, test_loader)\n",
    "    plt.plot(train_losses, label=f\"ADAM lr={lr}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss vs. Training Step\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1BjghDmBKmE"
   },
   "source": [
    "### Display model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "praSRn54BKmE"
   },
   "outputs": [],
   "source": [
    "## Display model predictions\n",
    "## Generic function to display predictions for a few images\n",
    "\n",
    "def display_predictions(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6buAMF5CBKmE"
   },
   "outputs": [],
   "source": [
    "# TODO Diplay your best model predictions\n",
    "model = MyFinalNet() ##TODO\n",
    "model = model.to(device)\n",
    "display_predictions(model, num_images=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-A_mLSoVO9O"
   },
   "source": [
    "## Task 5: Feature Visualization (5 points)\n",
    "Visualize feature maps of the first and the last convolutional layer of your final network using **cifar_example.jpg** as input image. Show the visualization in the report.\n",
    "\n",
    "#### First layer activations\n",
    "<img src=\"https://i.imgur.com/kGB9AuP.png\" width=\"400\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsbbsIGKVO9P"
   },
   "source": [
    "#### Last layer activations\n",
    "\n",
    "<img src=\"https://i.imgur.com/qelH05X.png\" width=\"400\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IhQxWX8RVO91"
   },
   "outputs": [],
   "source": [
    "#Task 5: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNX3eVEyVO94"
   },
   "outputs": [],
   "source": [
    "def transfer_single_img_to_tensor(img_path):\n",
    "    im = Image.open(img_path)\n",
    "    img = np.asarray(im)/255\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    \n",
    "    inp = (img - mean) / std\n",
    "    inp = np.asarray(inp, dtype=np.float32)\n",
    "    inp = inp.transpose((2, 0, 1))\n",
    "    inp = np.expand_dims(inp, axis=0)\n",
    "    inp = torch.from_numpy(inp, )\n",
    "    inputs = inp.to(device)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4Qy8s4BVO9_"
   },
   "outputs": [],
   "source": [
    "def feature_imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.detach().numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0AqVUu9VO97"
   },
   "outputs": [],
   "source": [
    "inputs = transfer_single_img_to_tensor('example_imgs/cifar_example.jpg') # Loads an image and normalizes it\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "   # TODO: Retrive the first and the last layer feature maps of your best model. (Hint: Move back to CPU)\n",
    "    \n",
    "    # Load the best model\n",
    "    best_model = torch.load(\"best_model.pth\")\n",
    "\n",
    "#     def get_feature_maps(model, input):\n",
    "#         # Define empty lists to store the feature maps\n",
    "#         first_layer_maps = []\n",
    "#         last_layer_maps = []\n",
    "\n",
    "#         # Define the hooks for the first and last layers\n",
    "#         def first_layer_hook(module, input, output):\n",
    "#             first_layer_maps.append(output.detach().cpu())\n",
    "#         def last_layer_hook(module, input, output):\n",
    "#             last_layer_maps.append(output.detach().cpu())\n",
    "\n",
    "#         # Register the hooks\n",
    "#         model.conv1.register_forward_hook(first_layer_hook)\n",
    "#         model.fc3.register_forward_hook(last_layer_hook)\n",
    "\n",
    "#         # Run the model on the input\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             model(input)\n",
    "\n",
    "#         # Remove the hooks\n",
    "#         model.conv1.unregister_forward_hook(first_layer_hook)\n",
    "#         model.fc3.unregister_forward_hook(last_layer_hook)\n",
    "\n",
    "#         # Return the feature maps\n",
    "#         return first_layer_maps1, last_layer_maps1\n",
    "\n",
    "    # Get the feature maps\n",
    "    first_layer_maps, last_layer_maps = get_feature_maps(best_model, input) ##HUI TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ofvGE8-VO-B"
   },
   "outputs": [],
   "source": [
    "# Visualize the feature maps\n",
    "out = torchvision.utils.make_grid(first_layer_maps, normalize=True, scale_each=True)\n",
    "feature_imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Mo1xtSzVO-E"
   },
   "outputs": [],
   "source": [
    "out = torchvision.utils.make_grid(last_layer_maps, normalize=True, scale_each=True)\n",
    "feature_imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 0 1\n",
      "False\n",
      "16 16\n",
      "-- 1 1\n",
      "True\n",
      "16 32\n",
      "-- 2 1\n",
      "True\n",
      "32 64\n",
      "[[ 0.03614257 -0.11447922 -0.00608285 ...  0.09562786 -0.05837232\n",
      "   0.00630627]\n",
      " [ 0.03315374 -0.03426623  0.05281999 ...  0.04018801  0.0363416\n",
      "   0.0021361 ]\n",
      " [-0.06088167  0.08568906 -0.05251234 ... -0.14975338  0.18463183\n",
      "  -0.02893766]\n",
      " ...\n",
      " [-0.20379863  0.19193744  0.18737518 ... -0.18271653 -0.21279135\n",
      "  -0.19107082]\n",
      " [ 0.03579932 -0.0971399  -0.07980046 ... -0.03927716 -0.00694625\n",
      "  -0.09953044]\n",
      " [-0.03674747 -0.05493688 -0.2050911  ...  0.04234331  0.0307241\n",
      "   0.02567111]]\n",
      "[[ 0.02746231 -0.16358249  0.05600756 ...  0.11009977  0.01260974\n",
      "   0.11766019]\n",
      " [ 0.0060675  -0.00405898  0.04668303 ... -0.07084221 -0.00099888\n",
      "  -0.02177241]\n",
      " [-0.0760075   0.0621215  -0.15747666 ...  0.06615362  0.0131195\n",
      "   0.15038898]\n",
      " ...\n",
      " [-0.2369792   0.09459271  0.04741216 ... -0.0082923   0.1809585\n",
      "  -0.01551003]\n",
      " [-0.05940694  0.01231841  0.11214063 ...  0.07280646 -0.08228176\n",
      "   0.03651057]\n",
      " [ 0.11017407 -0.0932294   0.17555126 ...  0.05834725 -0.00284723\n",
      "   0.0560923 ]]\n",
      "[[-2.14623457e-01 -1.06560562e-01 -1.79762873e-02 ...  1.24577151e-01\n",
      "  -9.24806530e-02 -1.07361036e-01]\n",
      " [-3.84389175e-02 -1.15154010e-02  3.45298473e-02 ...  1.58928431e-02\n",
      "   2.82756172e-02 -4.90133646e-02]\n",
      " [ 9.29432423e-02 -5.80077231e-02  9.67113351e-02 ...  3.79851953e-02\n",
      "  -1.25591791e-01 -2.56978062e-01]\n",
      " ...\n",
      " [ 8.22338152e-02 -2.12260156e-05  1.40448866e-01 ...  1.21528088e-01\n",
      "  -1.52380287e-01  6.69688305e-02]\n",
      " [-2.01681564e-01  1.63037368e-01 -1.18353302e-01 ...  1.25807080e-02\n",
      "  -1.52749110e-01 -4.42172534e-02]\n",
      " [-3.29301069e-02 -8.25016347e-02  1.11463153e-01 ... -4.48568663e-02\n",
      "   6.14004069e-02  1.40030885e-01]]\n",
      "[[ 0.00240963  0.12729297 -0.13998327 ... -0.04846292 -0.02942065\n",
      "   0.11631984]\n",
      " [ 0.05106141  0.1443779   0.01675951 ...  0.00465215  0.00120869\n",
      "  -0.12283374]\n",
      " [-0.11514585  0.06904843 -0.13859534 ... -0.18697613  0.09730956\n",
      "  -0.10313505]\n",
      " ...\n",
      " [-0.0958526   0.18075776 -0.24874049 ... -0.146818    0.02221157\n",
      "   0.0431782 ]\n",
      " [-0.1063741   0.1020797   0.10299753 ...  0.08020744  0.13152282\n",
      "  -0.03902939]\n",
      " [ 0.00824419 -0.0817495  -0.07469564 ... -0.16366375 -0.02745603\n",
      "   0.11456914]]\n",
      "[[ 0.15828214  0.09299233  0.04024687 ...  0.13919573  0.08122917\n",
      "   0.13338321]\n",
      " [ 0.14004325  0.12908141 -0.02638783 ...  0.00694536 -0.06599408\n",
      "  -0.13222028]\n",
      " [-0.07372498  0.04534132  0.11866359 ...  0.08796531  0.0059372\n",
      "  -0.27487942]\n",
      " ...\n",
      " [ 0.15270088 -0.05135748  0.07257272 ... -0.05693658  0.01307431\n",
      "   0.073501  ]\n",
      " [ 0.05022413  0.11222842  0.03397464 ...  0.07231022  0.00684981\n",
      "   0.21971614]\n",
      " [ 0.01782494 -0.04542173 -0.10967467 ... -0.0662222  -0.06539796\n",
      "   0.04578097]]\n",
      "[[ 0.1333013  -0.00809153 -0.01745712 ... -0.02028079  0.106538\n",
      "   0.22415373]\n",
      " [-0.06743596  0.09191435  0.01421476 ... -0.07306106  0.07791194\n",
      "   0.15236153]\n",
      " [ 0.15793488 -0.0893058  -0.21452686 ... -0.15374252  0.0547217\n",
      "  -0.03764006]\n",
      " ...\n",
      " [-0.07584287  0.09974223  0.03047796 ... -0.06179408  0.09888246\n",
      "  -0.07066503]\n",
      " [ 0.04337562  0.01777795  0.14813395 ... -0.10498178 -0.05641815\n",
      "   0.01455372]\n",
      " [-0.09474141 -0.02840849 -0.08207815 ...  0.01502624 -0.12140388\n",
      "  -0.06043035]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "c_hidden=[16,32,64]\n",
    "for block_idx, block_count in enumerate([1,1,1]):\n",
    "    print('--',block_idx, block_count)\n",
    "    for bc in range(block_count):\n",
    "        subsample = (bc == 0 and block_idx > 0) # Subsample the first block of each group, except the very first one.\n",
    "        print(subsample)\n",
    "        c_in=c_hidden[block_idx if not subsample else (block_idx-1)]\n",
    "        c_out=c_hidden[block_idx]\n",
    "        print(c_in, c_out)\n",
    "import numpy as np\n",
    "dims=[100]*7\n",
    "for din, dout in zip(dims[:-1], dims[1:]):\n",
    "    w=np.random.randn(din,dout)/np.sqrt(din)\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Weight Visualization (5 points)\n",
    "Visualize weights of convolutional layers of your final network. Show the visualization in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Task 6\n",
    "# Hint: \n",
    "# 1) What is the size of each weight (filter) tensor?\n",
    "# 2) Normalize weight values to [0, 1]\n",
    "# 3) Each subfigure should be of size [kernel_size, kernel_size]\n",
    "# 4) How many subfigures in total?\n",
    "\n",
    "# Get the convolutional layers\n",
    "# conv_layers = 1 ##HUITODO\n",
    "\n",
    "# # Plot the weights of each convolutional layer\n",
    "# for i, layer in enumerate(conv_layers):\n",
    "#     weights = layer.weight.data\n",
    "#     num_kernel = weights.shape2 * weights.shape3\n",
    "#     fig, axs = plt.subplots(weights.shape2, weights.shape3, figsize=(kernel_size, kernel_size))\n",
    "#     for j in range(weights.shape2):\n",
    "#         for k in range(weights.shape3):\n",
    "#             # Normalize the weight values to 4\n",
    "#             weight = weights5.cpu().numpy()\n",
    "#             weight = (weight - weight.min()) / (weight.max() - weight.min())\n",
    "#             axs5.imshow(weight, cmap=\"gray\")\n",
    "#     axs5.axis(\"off\")\n",
    "#     fig.suptitle(f\"Convolutional Layer {i+1}\")\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_dists(val_dict, color=\"C0\", xlabel=None, stat=\"count\", use_kde=True):\n",
    "    columns = len(val_dict)\n",
    "    fig, ax = plt.subplots(1, columns, figsize=(columns*3, 2.5))\n",
    "    fig_index = 0\n",
    "    for key in sorted(val_dict.keys()):\n",
    "        key_ax = ax[fig_index%columns]\n",
    "        sns.histplot(val_dict[key], ax=key_ax, color=color, bins=50, stat=stat,\n",
    "                     kde=use_kde and ((val_dict[key].max()-val_dict[key].min())>1e-8)) # Only plot kde if there is variance\n",
    "        key_ax.set_title(f\"{key} \" + (r\"(%i $\\to$ %i)\" % (val_dict[key].shape[1], val_dict[key].shape[0]) if len(val_dict[key].shape)>1 else \"\"))\n",
    "        if xlabel is not None:\n",
    "            key_ax.set_xlabel(xlabel)\n",
    "        fig_index += 1\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "    return fig\n",
    "\n",
    "def normalize_weights(conv_layer):\n",
    "    \"Normalize the weights\"\n",
    "    weights = conv_layer.weight.data\n",
    "    min_val = weights.min().item()\n",
    "    max_val = weights.max().item()\n",
    "    weights = (weights - min_val) / (max_val - min_val)\n",
    "    return weights\n",
    "\n",
    "def equal_var_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        else:\n",
    "            #a = param.detach().numpy().shape[1]\n",
    "            a = 3\n",
    "            param.data.normal_(std=1.0/math.sqrt(a)) ##param.shape[1]\n",
    "\n",
    "def visualize_weight_distribution(model, color=\"C0\"):\n",
    "    weights = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            continue\n",
    "        key_name = f\"Layer {name.split('.')[1]}\"\n",
    "        weights[key_name] = param.detach().view(-1).cpu().numpy()\n",
    "\n",
    "    ## Plotting\n",
    "    fig = plot_dists(weights, color=color, xlabel=\"Weight vals\")\n",
    "    fig.suptitle(\"Weight distribution\", fontsize=14, y=1.05)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "equal_var_init(best_model)\n",
    "visualize_weight_distribution(best_model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
