{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pkkGX0S0oO3N"
   },
   "source": [
    "# Project: Dense Prediction: Monocular Depth Estimation and Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total points in the project = 120 = 65 (Part 1) + 35 (Part 2) + 20 (Competition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://i.imgur.com/I2rSgxd.png' width=200> <img src='https://i.imgur.com/1oP2EIg.png' width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Carefully!\n",
    "\n",
    "<hr/>\n",
    "\n",
    "**Note**: If you are using Google Colab, Make sure that your Colab notebook is a GPU instance. Also, the first time you run the training, the instance might crash for exceeding the allocated memory. This is expected behaviour, especially with large batch sizes. Collab will suggest restarting the session and providing instances with larger memory sizes.\n",
    "\n",
    "**Note**: This project is more open-ended. Multiple solutions can be considered _correct_. As there already exist implementations of various deep networks for this task on the interweb, **plagiarism will NOT be tolerated**. Your code will be judged for similarity against code available online and other students' code. You are expected to justify every design decision when your project is being evaluated. Any plagiarism detected will lead to a Grade of ZERO WITHOUT ANY WARNINGS. You will be explicitly guided in the notebook to look-up information on internet when needed. \n",
    "\n",
    "**Note**: The networks you will design/implement may be much larger than what you have previously designed. Please bring hardware concerns to the attention to the TAs on slack as required. You will need to begin early to test out new ideas/hyperparameters and training will take much longer. Best of luck!\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competition [20 points]\n",
    "This project contains two sub-parts (Depth estimation and semantic segmentation) for which you will develop your models. Performance of your models on both the sub-tasks (given validation sets) will be compared against the other students in the class and ranked in the leaderboard. 20 points for grading are reserved for the competition and will be given depending on your position on the leaderboard. Your notebook will be re-run and your pretrained models may also be chosen for further testing for verification. Any form of cheating WILL be caught and will result in TOTAL grade of ZERO without any warnings.\n",
    "\n",
    "#### **Top 3 winners** will be given food coupons by Prof. Peter Wonka :\\)\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8w1z-r-IoO3P"
   },
   "source": [
    "# Part 1 : Monocular Depth Estimation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "- In this part of the project, you are tasked to create a model that **estimates depth from a single input image**. The input is an RGB image and the output is a single channel dense depth map where each pixel is the estimated distance from the 'camera sensor' to an object in the scene in real world units (e.g. in meters). Depth from a single image is a fundemental vision task with many useful applications including scene understanding and reconstruction.\n",
    "\n",
    "- You are to develop a convolutional neural network (CNN) that formulates the problem as a regression of the depth map from a single RGB image. \n",
    "\n",
    "- In this section, we provide all the source code needed for loading and evaluating your model.  You will reuse the model in the next section\n",
    "\n",
    "- Your task in this section is to modify the code in order to:\n",
    "    - Define a [UNet](https://arxiv.org/abs/1505.04597) model that takes an RGB image and outputs a single channel depth map. **[25 points]**\n",
    "    - Define an approprate loss function. **[15 points]**\n",
    "    - Tune the model to achieve an RMSE of **0.035** or less on the given validation set. **[25 points]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "  Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting timm\n",
      "  Obtaining dependency information for timm from https://files.pythonhosted.org/packages/7a/bd/2c56be7a3b5bc71cf85a405246b89d5359f942c9f7fb6db6306d9d056092/timm-0.9.7-py3-none-any.whl.metadata\n",
      "  Downloading timm-0.9.7-py3-none-any.whl.metadata (58 kB)\n",
      "     ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "     -------------------- ------------------- 30.7/58.8 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 58.8/58.8 kB 771.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from datasets) (1.25.2)\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=8.0.0 from https://files.pythonhosted.org/packages/ff/9e/4e2f6292616fed075d4bdf2a69d01cfa35fe3fcff52155d548c333e0a169/pyarrow-13.0.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-13.0.0-cp39-cp39-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/30/25/ae5ede30b61e78d98cc0be2e94ad8a942ced11eafba0c2d2efe0db7647aa/xxhash-3.3.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.3.0-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<2023.9.0,>=2023.1.0 (from datasets)\n",
      "  Obtaining dependency information for fsspec[http]<2023.9.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/c2/7c/66aff492b444f0d089bd74ffcb7346ebc3521ba68c77ac5479a2b947091c/aiohttp-3.8.5-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading aiohttp-3.8.5-cp39-cp39-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
      "  Obtaining dependency information for huggingface-hub<1.0.0,>=0.14.0 from https://files.pythonhosted.org/packages/50/9d/5eac2733606df7d164b951b14cd76b056e530af96c881aaec89383bdbe45/huggingface_hub-0.17.1-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from datasets) (23.1)\n",
      "Collecting pyyaml>=5.1 (from datasets)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/84/4d/82704d1ab9290b03da94e6425f5e87396b999fd7eb8e08f3a92c158402bf/PyYAML-6.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: torch>=1.7 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from timm) (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from timm) (0.15.0)\n",
      "Collecting safetensors (from timm)\n",
      "  Obtaining dependency information for safetensors from https://files.pythonhosted.org/packages/d8/c8/74f442188c34f40ae1a7d775ef090ac7451656282871977229d6db52e5ce/safetensors-0.3.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp39-cp39-win_amd64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.2-cp39-cp39-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 0.0/61.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.7/61.7 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/f9/16/ef36f5b20ee10dba86d4b5223d55b416e97dfa2dbf5546f0c6d9aa8a26ba/frozenlist-1.4.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading frozenlist-1.4.0-cp39-cp39-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: sympy in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from torch>=1.7->timm) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from torch>=1.7->timm) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from torch>=1.7->timm) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from torchvision->timm) (9.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from jinja2->torch>=1.7->timm) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\wangh0m\\appdata\\local\\anaconda3\\envs\\dlvc\\lib\\site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "   ---------------------------------------- 0.0/519.6 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 112.6/519.6 kB 3.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 204.8/519.6 kB 2.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 276.5/519.6 kB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 501.8/519.6 kB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 519.6/519.6 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 16.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 10.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 10.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.3/115.3 kB 7.0 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.8.5-cp39-cp39-win_amd64.whl (327 kB)\n",
      "   ---------------------------------------- 0.0/327.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 327.1/327.1 kB 10.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 294.8/294.8 kB 9.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-13.0.0-cp39-cp39-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/24.4 MB 10.2 MB/s eta 0:00:03\n",
      "   ---------------------------------------- 0.2/24.4 MB 10.2 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.6/24.4 MB 12.4 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 2.0/24.4 MB 10.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 2.2/24.4 MB 11.4 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 2.7/24.4 MB 10.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 9.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.5/24.4 MB 9.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 3.9/24.4 MB 9.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.2/24.4 MB 9.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.5/24.4 MB 9.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.9/24.4 MB 8.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.2/24.4 MB 8.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.3/24.4 MB 8.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 5.8/24.4 MB 8.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.1/24.4 MB 8.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.3/24.4 MB 8.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.5/24.4 MB 7.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 6.8/24.4 MB 7.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.0/24.4 MB 7.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.2/24.4 MB 7.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.5/24.4 MB 7.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.7/24.4 MB 7.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.0/24.4 MB 7.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.2/24.4 MB 7.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.5/24.4 MB 6.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 8.7/24.4 MB 7.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 8.9/24.4 MB 6.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.2/24.4 MB 6.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.3/24.4 MB 6.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.5/24.4 MB 6.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 9.8/24.4 MB 6.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.0/24.4 MB 6.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.2/24.4 MB 6.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.4/24.4 MB 6.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.6/24.4 MB 6.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.7/24.4 MB 6.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.9/24.4 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.1/24.4 MB 6.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.3/24.4 MB 5.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.5/24.4 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.7/24.4 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.9/24.4 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.0/24.4 MB 5.4 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.2/24.4 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.2/24.4 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.6/24.4 MB 5.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 12.7/24.4 MB 5.2 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.8/24.4 MB 5.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.9/24.4 MB 5.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.9/24.4 MB 5.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.1/24.4 MB 4.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.3/24.4 MB 4.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.4/24.4 MB 4.7 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.5/24.4 MB 4.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.6/24.4 MB 4.5 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.7/24.4 MB 4.5 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.8/24.4 MB 4.4 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 13.9/24.4 MB 4.3 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 14.0/24.4 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.1/24.4 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.2/24.4 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.3/24.4 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.4/24.4 MB 4.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.5/24.4 MB 4.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.6/24.4 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.7/24.4 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.8/24.4 MB 3.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.9/24.4 MB 3.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 15.1/24.4 MB 3.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 15.2/24.4 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.3/24.4 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.4/24.4 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.5/24.4 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.6/24.4 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.7/24.4 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.8/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 15.9/24.4 MB 3.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.0/24.4 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.1/24.4 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.2/24.4 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.3/24.4 MB 3.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.5/24.4 MB 3.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.6/24.4 MB 3.3 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.7/24.4 MB 3.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.8/24.4 MB 3.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.9/24.4 MB 3.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.0/24.4 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.1/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.2/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.2/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.4/24.4 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.5/24.4 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.6/24.4 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.7/24.4 MB 3.0 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 17.8/24.4 MB 2.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 17.9/24.4 MB 2.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 17.9/24.4 MB 2.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.0/24.4 MB 2.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.1/24.4 MB 2.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.2/24.4 MB 2.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.3/24.4 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.4/24.4 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.5/24.4 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.5/24.4 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.5/24.4 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.7/24.4 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.8/24.4 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.8/24.4 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 18.9/24.4 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.0/24.4 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.0/24.4 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.4 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.1/24.4 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.2/24.4 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.3/24.4 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.3/24.4 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.4/24.4 MB 2.4 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 19.5/24.4 MB 2.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 19.5/24.4 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.6/24.4 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.7/24.4 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.7/24.4 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.8/24.4 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.4 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.4 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.4 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.1/24.4 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.1/24.4 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.2/24.4 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.2/24.4 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.3/24.4 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.3/24.4 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.4/24.4 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.4/24.4 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.5/24.4 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.5/24.4 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.6/24.4 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.7/24.4 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.7/24.4 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 20.7/24.4 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 20.8/24.4 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 20.9/24.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 20.9/24.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.0/24.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.1/24.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.3/24.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.4/24.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.4/24.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.5/24.4 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.6/24.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.7/24.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.7/24.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.7/24.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.8/24.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.4 MB 1.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 21.9/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.0/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.0/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.1/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.1/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.2/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.2/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.3/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.4/24.4 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.4 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.4 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.5/24.4 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.6/24.4 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.6/24.4 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.6/24.4 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.6/24.4 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.7/24.4 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.7/24.4 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.7/24.4 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.7/24.4 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.8/24.4 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.8/24.4 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.8/24.4 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.8/24.4 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.9/24.4 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.9/24.4 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 22.9/24.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.0/24.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.0/24.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.0/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.1/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.2/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.2/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.3/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.4/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.4/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.4 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.5/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.8/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.8/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.0/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.2/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.2/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.8 kB ? eta -:--:--\n",
      "   ---------- ---------------------------- 41.0/152.8 kB 653.6 kB/s eta 0:00:01\n",
      "   -------------------- ------------------ 81.9/152.8 kB 919.0 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 112.6/152.8 kB 930.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 143.4/152.8 kB 853.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- 152.8/152.8 kB 828.6 kB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.3 kB ? eta -:--:--\n",
      "   ----------- --------------------------- 41.0/133.3 kB 991.0 kB/s eta 0:00:01\n",
      "   -------------------- ------------------ 71.7/133.3 kB 787.7 kB/s eta 0:00:01\n",
      "   -------------------------------------  133.1/133.3 kB 983.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 133.3/133.3 kB 879.4 kB/s eta 0:00:00\n",
      "Downloading safetensors-0.3.3-cp39-cp39-win_amd64.whl (266 kB)\n",
      "   ---------------------------------------- 0.0/266.4 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/266.4 kB 991.0 kB/s eta 0:00:01\n",
      "   ------------- -------------------------- 92.2/266.4 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 122.9/266.4 kB 1.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 184.3/266.4 kB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ----- 225.3/266.4 kB 986.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 266.4/266.4 kB 911.7 kB/s eta 0:00:00\n",
      "Downloading xxhash-3.3.0-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.0-cp39-cp39-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.7 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 41.0/44.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.7/44.7 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "   --------- ----------------------------- 41.0/163.8 kB 960.0 kB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 92.2/163.8 kB 1.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 143.4/163.8 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 163.8/163.8 kB 978.3 kB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, xxhash, tqdm, pyyaml, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, huggingface-hub, aiosignal, aiohttp, timm, datasets\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.14.5 dill-0.3.7 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.17.1 multidict-6.0.4 multiprocess-0.70.15 pyarrow-13.0.0 pyyaml-6.0.1 safetensors-0.3.3 timm-0.9.7 tqdm-4.66.1 xxhash-3.3.0 yarl-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm datasets timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "ds = datasets.load_dataset(\"shariqfarooq/cs323_densepred_depth\")  # DO NOT change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'depth'],\n",
       "        num_rows: 25356\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'depth'],\n",
       "        num_rows: 518\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "image_transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "depth_transforms = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "def transform(batch):\n",
    "  batch['image'] = ([image_transforms(im) for im in batch['image']])\n",
    "  batch['depth'] = [depth_transforms(d)[:1] for d in batch['depth']]\n",
    "  return batch\n",
    "\n",
    "ds.set_transform(transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 256, 256]), torch.Size([1, 128, 128]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['image'].shape, ds['train'][0]['depth'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "U5v0Gou2oO3X",
    "outputId": "ebcc88fc-9abd-45b2-e438-b3c2d16a32c7"
   },
   "outputs": [],
   "source": [
    "# Examine training data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def denorm_imagenet(x):\n",
    "    mean, std = torch.tensor(IMAGENET_MEAN).to(x.device), torch.tensor(IMAGENET_STD).to(x.device)\n",
    "    if x.ndim == 3:\n",
    "        mean, std = mean[:, None, None], std[:, None, None]\n",
    "    elif x.ndim == 4:\n",
    "        mean, std = mean[None, :, None, None], std[None, :,  None, None]\n",
    "\n",
    "    return x * std + mean\n",
    "\n",
    "\n",
    "def show_example_data(dataset, split='train', num=5):\n",
    "    im_stacked = []\n",
    "    depth_stacked = []\n",
    "    for i in range(num):\n",
    "        sample = dataset[split][i]\n",
    "        im_stacked.append(denorm_imagenet(sample['image']))\n",
    "        depth_stacked.append(sample['depth'])\n",
    "    \n",
    "    im_stacked = make_grid(torch.stack(im_stacked), nrow=num)\n",
    "    depth_stacked = make_grid(torch.stack(depth_stacked), nrow=num, normalize=True, scale_each=True)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    ax[0].imshow(im_stacked.permute(1, 2, 0))\n",
    "    ax[1].imshow(depth_stacked[0])\n",
    "    for a in ax:\n",
    "        a.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_example_data(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with timm\n",
    "\n",
    "[`timm`](https://timm.fast.ai/) is a popular deep-learning library created by [Ross Wightman](https://twitter.com/wightmanr) and is a collection of SOTA computer vision models, layers, utilities, optimizers, schedulers, data-loaders, augmentations and also training/validating scripts with ability to reproduce ImageNet training results. \n",
    "\n",
    "We will be using `timm` to load encoder backbones pretrained on ImageNet so we don't have to define encoder architectures (that you did in previous projects) or start training from scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm example for loading any encoder architecture\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "model = timm.create_model('resnet34') # load resnet34 architecture\n",
    "x     = torch.randn(1, 3, 224, 224)  # create random input tensor\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also load a pretrained model\n",
    "pretrained_resnet_34 = timm.create_model('resnet34', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List al available models\n",
    "avail_pretrained_models = timm.list_models(pretrained=True)\n",
    "\n",
    "print(f\"Number of available pretrained models: {len(avail_pretrained_models)}\")\n",
    "print(f\"First 5 models: {avail_pretrained_models[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have vast variety of available pretrained models (750+)! In this project, you are free to choose any one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching through 750+ models can be a pain. timm provides glob filtering so we can filter the models by name\n",
    "# Lets list all the models that have 'efficientnet' in their name\n",
    "all_efficientnet_models = timm.list_models('*efficientnet*', pretrained=True)\n",
    "all_efficientnet_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction with timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = \"resnet34\"  # TODO: Change this to your desired backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tell timm that we only need features from various from various layers (and not final ImageNet class logits)\n",
    "encoder = timm.create_model(BACKBONE, pretrained=True, features_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Channel info:\", encoder.feature_info.channels())\n",
    "x = torch.rand(1, 3, 224, 224)\n",
    "out = encoder(x)\n",
    "assert type(out) is list\n",
    "[o.shape for o in out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that now we are able to get multi-scale features from our encoder. You will use this to build your Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNSSgqhEoO3a"
   },
   "source": [
    "## Model [25 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm import create_model\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    TODO: Complete the docstring of the Unet class (description, parameters, returns, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone='resnet50',\n",
    "            num_classes=1,\n",
    "            final_activation=nn.Identity(), # TODO: Change this to your desired final activation\n",
    "            # TODO: Add any other relevant args you need\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO: Complete the definition of the Unet class\n",
    "        # Use timm to load the encoder backbone with imagenet pretrained weights\n",
    "        # encoder **must** return a list of feature maps that will be used by the decoder\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.final_activation = final_activation\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # TODO: Complete the forward function\n",
    "        x = None\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(BACKBONE, num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always perform a sanity check on the models you define\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "out = model(x)\n",
    "print(\"Input shape\", x.shape)\n",
    "print(\"Output shape\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to GPUs. Using all GPUs available by default. You can change this.\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4DTq8InPoO3d"
   },
   "source": [
    "## Loss Function [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss function that is suitable for the depth estimation. Look up the latest papers, for example on [PapersWithCode leaderboards](https://paperswithcode.com/sota/monocular-depth-estimation-on-nyu-depth-v2).\n",
    "Why will the current loss not work? Submit the answer in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g19R7WBboO3e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def loss_fn(pred_y, y):\n",
    "    return torch.mean(y.sub(pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqhhpdZhoO3h"
   },
   "source": [
    "## Training + Evaluation [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the hyperparameters and the architecture to achieve the target RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "# TODO: Change these to your desired hyperparameters\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 32\n",
    "learning_rate = 10\n",
    "\n",
    "workers = 4 # The number of parallel processes used to read data. Increase this if you have more cores.\n",
    "train_loader = DataLoader(ds['train'], batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "test_loader = DataLoader(ds['test'], batch_size=batch_size, shuffle=False, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "tHUCEZcCoO3i",
    "outputId": "5dd810fb-86db-4daf-dd1b-bb2fbb5368ca"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "run_id = f'model_n{epochs}_bs{batch_size}_lr{learning_rate}'; print('\\n\\nTraining', run_id)\n",
    "save_path = run_id + '.pkl'\n",
    "\n",
    "# TODO: Experiment with different optimizers and learning rate schedulers\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "\n",
    "\n",
    "class RMSE(object):\n",
    "    def __init__(self):\n",
    "        self.sq_errors = []\n",
    "        self.num_pix = 0\n",
    "        \n",
    "    def get(self):\n",
    "        return np.sqrt(\n",
    "                    np.sum(np.array(self.sq_errors))/self.num_pix\n",
    "                )\n",
    "    \n",
    "    def add_batch(self, pred, target):\n",
    "        sqe = (pred-target)**2\n",
    "        self.sq_errors.append(np.sum(sqe))\n",
    "        self.num_pix += target.size\n",
    "        \n",
    "    def reset(self):\n",
    "        self.sq_errors = []\n",
    "        self.num_pix = 0\n",
    "\n",
    "\n",
    "\n",
    "ITER_PER_EPOCH = len(train_loader)\n",
    "TOTAL_STEPS = ITER_PER_EPOCH * epochs\n",
    "\n",
    "\n",
    "metrics = RMSE()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, valid_loader):\n",
    "    model.eval()\n",
    "    metrics.reset()\n",
    "    for i, (sample) in tqdm(enumerate(valid_loader), total=len(valid_loader), desc='Validating'):\n",
    "        x, y = sample['image'].float().cuda(), sample['depth'].numpy()\n",
    "        y_pred = model(x).detach().cpu().numpy()\n",
    "        metrics.add_batch(y_pred, y)\n",
    "    print('\\nValidation RMSE {avg_rmse}'.format(avg_rmse=metrics.get()))\n",
    "\n",
    "\n",
    "# One validation before we start training (good practice to catch errors early)\n",
    "validate(model, test_loader)\n",
    "pbar = tqdm(total=TOTAL_STEPS, desc='Training')\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    N = len(train_loader)\n",
    "\n",
    "    for i, (sample) in enumerate(train_loader):\n",
    "\n",
    "        # Load a batch and send it to GPU\n",
    "        x = sample['image'].float().cuda()\n",
    "        y = sample['depth'].float().cuda()\n",
    "\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Compute and print loss.\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model).\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its parameters\n",
    "        optimizer.step()\n",
    "        # learning rate scheduler step.\n",
    "        # TODO: location of this call might change depending on your choice of scheduler. \n",
    "        scheduler.step()\n",
    "\n",
    "        pbar.update(1)\n",
    "        # Report progress. Add any extra logging info here\n",
    "        pbar.set_postfix({'epoch': f'{epoch+1}/{N}', 'loss': loss.item(), 'epoch%': \"{0:.1f}%\".format(100*(i+1)/N)})\n",
    "            \n",
    "        #break # useful for quick debugging        \n",
    "    torch.cuda.empty_cache(); del x, y; gc.collect()\n",
    "    \n",
    "    # Validation after each epoch\n",
    "    validate(model, test_loader)\n",
    "    \n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print('\\nTraining done. Model saved ({}).'.format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HzEZYQv5oO3l"
   },
   "source": [
    "## Visual Test of the Trained Model (no tasks required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "WtaiM5ekoO3m",
    "outputId": "efe6fddf-67b1-4536-aaab-c80185a32127"
   },
   "outputs": [],
   "source": [
    "# Load model from disk\n",
    "#model = Unet(BACKBONE, num_classes=1)\n",
    "#model.load_state_dict(torch.load('trained_model.pkl'))\n",
    "#model.eval() # set to evaluation mode\n",
    "\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "# Visualize validation sample\n",
    "sample = iter(test_loader).next()\n",
    "x = sample['image'].float().cuda()\n",
    "y_pred, y = model(x), sample['depth']\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "plt.imshow(make_grid(sample['image'], padding=0, normalize=True).permute((1, 2, 0)))\n",
    "plt.axis('off')\n",
    "plt.title(\"Input\")\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "plt.imshow(make_grid(sample['depth'], padding=0, normalize=True, scale_each=True).permute((1, 2, 0))[:,:,0])\n",
    "plt.axis('off')\n",
    "plt.title(\"Ground Truth\")\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(20,20))\n",
    "plt.imshow(make_grid(y_pred.detach().cpu(), padding=0, normalize=True, scale_each=True).permute((1, 2, 0))[:,:,0])\n",
    "plt.axis('off')\n",
    "plt.title(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  At this point, you can restart your notebook for part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Part 2 : Semantic Segmentation\n",
    "\n",
    "In this part of the project, you will reuse the model you created in the previous part to perform Semantic Segmentation - instead of assigning a real number to each\n",
    "pixel, you will assign it a class.\n",
    "\n",
    "The tasks are as following:\n",
    "- Implement data prepareation: encoding and decoding of segmentation maps **[5 points]**\n",
    "- Modify the UNet model that takes an RGB image and now outputs a _label map_ of _N_ classes **[15 points]**\n",
    "- Define an approprate loss function. **[3 points]**\n",
    "- Tune the model to achieve an mIOU of **0.45** or higher on the given validation set. **[10 points]**\n",
    "- Visualization **[2 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation [5 points]\n",
    "\n",
    "We are going to use the [PASCAL VOC dataset](http://host.robots.ox.ac.uk/pascal/VOC/), which is a commonly used benchmark. In order to reduce the\n",
    "computational requirements, we will be using [a variant](https://huggingface.co/datasets/shariqfarooq/cs323_densepred_seg256) that has a uniform and slightly lower resolution (256x256) than official. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load the dataset first\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds_voc = load_dataset(\"shariqfarooq/cs323_densepred_seg256\")  # DO NOT change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine training data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def make_pil_grid(images, nrow=8):\n",
    "    grid = Image.new('RGB', (images[0].width*nrow, images[0].height*((len(images)-1)//nrow+1)))\n",
    "    for i, im in enumerate(images):\n",
    "        grid.paste(im, (i%nrow*images[0].width, i//nrow*images[0].height))\n",
    "    return grid\n",
    "    \n",
    "\n",
    "\n",
    "def show_example_data(dataset, split='train', num=7, nrow=7):\n",
    "    ims = [dataset[split][i]['image'] for i in range(num)]\n",
    "    masks = [dataset[split][i]['mask'] for i in range(num)]\n",
    "    im_grid = make_pil_grid(ims, nrow)\n",
    "    mask_grid = make_pil_grid(masks, nrow)\n",
    "\n",
    "    grid = make_pil_grid([im_grid, mask_grid], 1)\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_example_data(ds_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to implement the encoding and decoding of the data.\n",
    "\n",
    "Make sure that the labels are in the range `0..N-1`, where\n",
    "N is the number of classes - 21 in our case. You can have one special label for unknown regions.\n",
    "\n",
    "We provide the map of RGB to label for convenience in `get_pascal_color_palette()`. The map should be read as this - if a pixel has color `[0, 0, 0]`, it has label 0. If the color is `[128, 0, 0]`, the label is 1 and so on.\n",
    "\n",
    "You need to use the palette information to implement `encode_segmap` and `decode_segmap` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pascal_color_palette():\n",
    "    \"\"\"Load the mapping that associates pascal classes with label colors\n",
    "    Returns:\n",
    "        np.ndarray with dimensions (21, 3)\n",
    "    \"\"\"\n",
    "    return np.asarray([[0, 0, 0],\n",
    "                       [128, 0, 0],\n",
    "                       [0, 128, 0],\n",
    "                       [128, 128, 0],\n",
    "                       [0, 0, 128],\n",
    "                       [128, 0, 128],\n",
    "                       [0, 128, 128],\n",
    "                       [128, 128, 128],\n",
    "                       [64, 0, 0],\n",
    "                       [192, 0, 0],\n",
    "                       [64, 128, 0],\n",
    "                       [192, 128, 0],\n",
    "                       [64, 0, 128],\n",
    "                       [192, 0, 128],\n",
    "                       [64, 128, 128],\n",
    "                       [192, 128, 128],\n",
    "                       [0, 64, 0],\n",
    "                       [128, 64, 0],\n",
    "                       [0, 192, 0],\n",
    "                       [128, 192, 0],\n",
    "                       [0, 64, 128]])\n",
    "\n",
    "def get_pascal_class_names():\n",
    "    return ['Background',\n",
    "            'Aeroplane',\n",
    "            'Bicycle',\n",
    "            'Bird',\n",
    "            'Boat',\n",
    "            'Bottle',\n",
    "            'Bus',\n",
    "            'Car',\n",
    "            'Cat',\n",
    "            'Chair',\n",
    "            'Cow',\n",
    "            'Diningtable',\n",
    "            'Dog',\n",
    "            'Horse',\n",
    "            'Motorbike',\n",
    "            'Person',\n",
    "            'Pottedplant',\n",
    "            'Sheep',\n",
    "            'Sofa',\n",
    "            'Train',\n",
    "            'Tvmonitor']\n",
    "\n",
    "\n",
    "def encode_segmap(mask, unk_label=255):\n",
    "    \"\"\"Encode segmentation label images as pascal classes\n",
    "    Args:\n",
    "        mask (np.ndarray or PIL.Image.Image): raw segmentation label image of dimension\n",
    "          (M, N, 3), in which the Pascal classes are encoded as colours.\n",
    "    Returns:\n",
    "        (np.ndarray): class map with dimensions (M,N), where the value at\n",
    "        a given location is the integer denoting the class index.\n",
    "    \"\"\"\n",
    "    # TODO: Complete this function\n",
    "    raise NotImplementedError(\"TODO: Implement this function\")\n",
    "\n",
    "def decode_segmap(mask, unk_label=255):\n",
    "    \"\"\"Decode segmentation label prediction as RGB images\n",
    "    Args:\n",
    "        mask (torch.tensor): class map with dimensions (B, M,N), where the value at\n",
    "        a given location is the integer denoting the class index.\n",
    "    Returns:\n",
    "        (np.ndarray): colored image of shape (BM, BN, 3)\n",
    "    \"\"\"\n",
    "    mask = mask.astype(int)\n",
    "    mask[mask == unk_label] = 0\n",
    "    # TODO: Complete this function\n",
    "    raise NotImplementedError(\"TODO: Implement this function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optionally add data augmentations to increase performance. \n",
    "# Note that any augmentation should act jointly on image and its mask label.  Look up `albumentations`.\n",
    "\n",
    "im_transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.4850, 0.4560, 0.4060], std=[0.2290, 0.2240, 0.2250])\n",
    "])\n",
    "\n",
    "\n",
    "def transform_voc(batch):\n",
    "    batch['image'] = [im_transforms(i) for i in batch['image']]\n",
    "    batch['mask'] = [torch.tensor(encode_segmap(m)) for m in batch['mask']]\n",
    "    return batch\n",
    "\n",
    "ds_voc.set_transform(transform_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement a few more sanity checks - the range of data in the RGB part, the range of data in the label part, whether the dataset returns tensors,\n",
    "whether the labels have the datatype `torch.long` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sanity tests as required\n",
    "# TODO: Add any sanity tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modifying Architecture and the loss [18 points]\n",
    "You will have to some form of surgery on the network you constructed in Part 1.\n",
    "You have to make sure you are initializing the weights with the depth model you trained above. And then doctor the model such that you attach a new _Segmentation Head_ as your final block.\n",
    "\n",
    "1. The number of channels the last layer predicts must change to the number of classes in the dataset. The last activation may also change. \n",
    "2. The loss function must change to reflect the fact that we are now performing per-pixel classification. (What loss did you use for classification in Project 1?)\n",
    "3. You might get a CUDA assert error. This means that you have a label higher than the number of channels in the _logits_. This is very common with semantic segmentation, where you might want to label some region unkown as it's label might be under doubt - for example near the edges of objects. Look up how to ignore a certain label with a classification loss.\n",
    "4. Take care of input, label and logit sizes. We want predictions to be 256x256 as well, so you may need an upsampling layer in the _Segmentation Head_\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "# TODO: Change these to your desired hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 1\n",
    "\n",
    "workers = 4 # The number of parallel processes used to read data. Increase this if you have more cores.\n",
    "train_loader = DataLoader(ds_voc['train'], batch_size=batch_size, shuffle=True, num_workers=workers)\n",
    "test_loader = DataLoader(ds_voc['val'], batch_size=batch_size, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT\n",
    "# You should use the exact same Unet class definition as in the previous section so\n",
    "# TODO: Run the cell containing the Unet class definition in the previous section before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = \"resnet34\"  # TODO: Change this to your desired backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained depth model\n",
    "depth_pretrained_path = \"my_trained_model.pkl\"  # TODO: Specify the path to your trained depth model\n",
    "model = Unet(BACKBONE, num_classes=1)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()\n",
    "model.load_state_dict(torch.load(depth_pretrained_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegHead(nn.Module):\n",
    "    pass # TODO: Implement a segmentation head here. Remember to complete the docstrings as well\n",
    "\n",
    "\n",
    "def change_classes(unet, target_classes=21):\n",
    "    seg_head = SegHead()\n",
    "    # TODO: Perform 'surgery' on the unet model to attach a segmentation head\n",
    "    return unet\n",
    "\n",
    "model = change_classes(model, target_classes=21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def loss_fn(pred_y, y):\n",
    "    #TODO: Change this to your desired loss function. \n",
    "    return torch.mean(y.sub(y_pred))  # Why wouldn't this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training and Evaluation [10 points]\n",
    "Tune the hyperparameters to get the maximum possible score on the PASCAL VOC challenge. \n",
    "And answer the following questions:\n",
    "1. What is the relationship between the _size_ of the class and the IOU How would you quantify this relationship?\n",
    "2. What is the relationship between the number of instances and the IOU? how many times a class exists in an image vs the IOU?\n",
    "3. Which weights can you not transfer from the depth model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils import Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_id = f'seg_model_n{epochs}_bs{batch_size}_lr{learning_rate}'; print('\\n\\nTraining', run_id)\n",
    "save_path = run_id + '.pkl'\n",
    "\n",
    "# TODO: Experiment with different optimizers and learning rate schedulers\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=learning_rate, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "\n",
    "metrics = Metrics(len(get_pascal_class_names()), get_pascal_class_names())\n",
    "\n",
    "\n",
    "ITER_PER_EPOCH = len(train_loader)\n",
    "TOTAL_STEPS = ITER_PER_EPOCH * epochs\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, valid_loader):\n",
    "    model.eval()\n",
    "    metrics.reset()\n",
    "    for i, (sample) in tqdm(enumerate(valid_loader), total=len(valid_loader), desc='Validating'):\n",
    "        x, y = sample['image'].float().cuda(), sample['mask'].numpy()\n",
    "        y_pred = model(x)\n",
    "        y_pred = torch.argmax(y_pred, dim=1) # get the most likely prediction\n",
    "        metrics.add_batch(y, y_pred.detach().cpu().numpy())\n",
    "    print('\\nValidation stats ', metrics.get_table())\n",
    "\n",
    "\n",
    "validate(model, test_loader)\n",
    "pbar = tqdm(total=TOTAL_STEPS, desc='Training')\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    N = len(train_loader)\n",
    "\n",
    "    for i, (sample) in enumerate(train_loader):\n",
    "\n",
    "        # Load a batch and send it to GPU\n",
    "        x = sample['image'].float().cuda()\n",
    "        y = sample['mask'].long().cuda()\n",
    "\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Compute and print loss.\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model).\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step() # TODO: location of this call might change depending on your choice of scheduler.\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({'epoch': f\"{epoch+1}/{epochs}\", 'loss': loss.item(), 'epoch%': \"{0:.1f}%\".format(100*(i+1)/N)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #break # useful for quick debugging\n",
    "    torch.cuda.empty_cache(); del x, y; gc.collect()\n",
    "\n",
    "    # Validation after each epoch\n",
    "    validate(model, test_loader)\n",
    "\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print('\\nTraining done. Model saved ({}).'.format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization  [2 points]\n",
    "Use the `decode_segmap` function to visualize images and their Ground Truth and Predicted segmentation maps. The images must be from the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement visualization"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Project_Depth_Estimate_good.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
